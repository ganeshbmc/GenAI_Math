{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"9ae31250","cell_type":"markdown","source":"# Beta VAE  ","metadata":{}},{"id":"0b74e4bb-5881-4ed9-9b7f-aab0629bd315","cell_type":"markdown","source":"## Steps  \n1. Import libraries  \n2. Prepare data  \n   ```Download  |  Transform  |  Dataloader```  \n3. Define parameters  \n   ```Model  |  Optimizer  |  Loss  |  Training  ```\n4. Build Model  \n   ```Components  ```\n5. Training loop  \n6. Visualize results  ","metadata":{}},{"id":"a4e0d705-b86a-4341-8e15-74d232231452","cell_type":"markdown","source":"## Import libraries  ","metadata":{}},{"id":"8085a938-b8cc-4733-a684-01813c3e0858","cell_type":"code","source":"# Import libraries\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision.utils import save_image\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nimport time\nimport datetime\n\nprint(f\"Imports completed at {datetime.datetime.now()}\")\n\n# Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Versions\nprint(f\"Torch: {torch.__version__}, TorchVision: {torchvision.__version__}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T16:43:13.757024Z","iopub.execute_input":"2025-08-22T16:43:13.757564Z","iopub.status.idle":"2025-08-22T16:43:27.270275Z","shell.execute_reply.started":"2025-08-22T16:43:13.757537Z","shell.execute_reply":"2025-08-22T16:43:27.269535Z"}},"outputs":[{"name":"stdout","text":"Imports completed at 2025-08-22 16:43:27.177932\nUsing device: cuda\nTorch: 2.6.0+cu124, TorchVision: 0.21.0+cu124\n","output_type":"stream"}],"execution_count":1},{"id":"b4fb1f6e-eedb-4a7e-b4f3-02ca806b721c","cell_type":"markdown","source":"## Define parameters  ","metadata":{}},{"id":"4eac0b38-2cd1-4c05-aeea-f7676e24d659","cell_type":"code","source":"# Set seed for PyTorch\nseed = 42\ntorch.manual_seed(seed)\n\n# Data prep params\nbatch_size = 128\n\n# Model params\nlatent_dim = 20\n\n# Optimizer params\nlearning_rate = 0.0002\n# beta1 = 0.5  # Adam optimizer beta1\n\n# # Loss params\n# criterion = nn.BCELoss()\n\n# Training params\nnum_epochs = 100","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T16:43:48.435811Z","iopub.execute_input":"2025-08-22T16:43:48.436714Z","iopub.status.idle":"2025-08-22T16:43:48.450214Z","shell.execute_reply.started":"2025-08-22T16:43:48.436684Z","shell.execute_reply":"2025-08-22T16:43:48.449461Z"}},"outputs":[],"execution_count":2},{"id":"bb24207a-d509-4906-845e-6ba7386a979d","cell_type":"markdown","source":"## Prepare data  ","metadata":{}},{"id":"04e2c8d4-2e75-4220-bf0b-5d7eed3a4f3f","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"e5e33335-93ff-4f27-b361-09c98c2c40b3","cell_type":"markdown","source":"## Build Model  \n```Beta VAE```  \n\n**Probabilistic Neural Networks**  \n\n- Encoder\n    - Takes image, outputs mean and log var of latent space dim  \n- Reparametrization (z-sampler)\n    - Takes the mean and log var output by the Encoder and creates new z using epsilon (latent dim) from std normal  \n- Decoder\n    - Takes the z and outputs mean of x_hat (note that the var of x_hat is 1)  ","metadata":{}},{"id":"095fa2db-ca3b-46fb-b1c0-bcf7a53824f3","cell_type":"code","source":"class BetaVAE(nn.Module):\n    def __init__(self, latent_dim=20):\n        super(BetaVAE, self).__init__()\n        self.latent_dim = latent_dim\n\n        #--------------Encoder--------------\n        self.encoder = nn.Sequential(\n            # Input: (B, 1, 28, 28)\n            nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1),\n            nn.ReLU(True),\n            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),\n            nn.ReLU(True),\n            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n            nn.ReLU(True),\n        )\n        self.flatten = nn.Flatten()\n        self.mu = nn.Linear(128 * 4 * 4, self.latent_dim)\n        self.logvar = nn.Linear(128 * 4 * 4, self.latent_dim)\n\n        #--------------Decoder--------------\n        self.fc_decode = nn.Linear(self.latent_dim, 128*4*4)\n        self.decoder = nn.Sequential(\n            # Input: z (B, latent_dim)\n            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1),\n            nn.Sigmoid,\n        )\n\n    def encode(self, x):\n        x = self.encoder(x)\n        x_flat = self.flatten()\n        mu = self.mu(x_flat)\n        logvar = self.logvar(x_flat)\n        return mu, logvar\n\n    def reparametrize():\n        pass\n\n    def decode():\n        pass\n\n    def forward():\n        pass","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"6eea0e57-61cb-436a-94c0-1967eb67afd2","cell_type":"markdown","source":"## Set up Optimizers  ","metadata":{}},{"id":"bc17d46d-6619-4fbd-a456-625a7ac4294f","cell_type":"markdown","source":"## Set up Loss functions  ","metadata":{}},{"id":"f2416e28-5968-499a-8a95-42af87185309","cell_type":"markdown","source":"## Code for visualizations  ","metadata":{}},{"id":"8f4bf6d3-c00a-4b90-8a36-4e349da1b381","cell_type":"markdown","source":"## Training loop  \n- Visualize results every 10 epochs  ","metadata":{}},{"id":"af7cfae9-a77e-4532-9358-3b4d219eb700","cell_type":"markdown","source":"## Training  ","metadata":{}}]}