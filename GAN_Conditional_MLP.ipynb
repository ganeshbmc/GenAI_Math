{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ganeshbmc/GenAI_Math/blob/main/GAN_Conditional_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7Fw9r_Q0FnE"
      },
      "source": [
        "## Steps  \n",
        "1. Import libraries  \n",
        "2. Prepare data  \n",
        "   ```Download  |  Transform  |  Dataloader```  \n",
        "3. Define parameters  \n",
        "   ```Model  |  Optimizer  |  Loss  |  Training  ```\n",
        "4. Build Model  \n",
        "   ```Components  ```\n",
        "5. Training loop  \n",
        "6. Visualize results  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_9EKk6A3gUO"
      },
      "source": [
        "## Import libraries  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKeOhWPDzyTC",
        "outputId": "e50e0ec9-c5ff-486f-a518-5b7f866b7793"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Imports completed at 2025-08-21 09:12:55.111143\n",
            "Using device: cuda\n",
            "Torch: 2.6.0+cu124, TorchVision: 0.21.0+cu124\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "print(f\"Imports completed at {datetime.datetime.now()}\")\n",
        "\n",
        "# Device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Versions\n",
        "print(f\"Torch: {torch.__version__}, TorchVision: {torchvision.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create output folder\n",
        "folder_name = \"cgan_mlp_generated_images\"\n",
        "os.makedirs(f\"{folder_name}\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxCuW4RgM4Qg"
      },
      "source": [
        "## Define parameters  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrAIgkbmM0Jb"
      },
      "outputs": [],
      "source": [
        "# Set seed for PyTorch\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# Data prep params\n",
        "batch_size = 128\n",
        "\n",
        "# Model params\n",
        "z_dim = 100     # z_dim\n",
        "img_shape = (1, 28, 28)         # MNIST size\n",
        "num_classes = 10                # New parameter compared to Vanilla GAN\n",
        "\n",
        "# Optimizer params\n",
        "learning_rate = 0.\n",
        "# beta1 = 0.5\n",
        "\n",
        "# # Loss params\n",
        "# criterion = nn.BCELoss()\n",
        "\n",
        "# Training params\n",
        "num_epochs = 20\n",
        "generator_rounds = 1\n",
        "discriminator_rounds = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# torch.prod(torch.tensor(img_shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tf-hpM2r3lJw"
      },
      "source": [
        "## Prepare data  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4rOxJbJ3XjQ",
        "outputId": "23e8f82e-393c-4804-ac24-d59def277239"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transform to be applied:\n",
            "Compose(\n",
            "    ToTensor()\n",
            "    Normalize(mean=(0.5,), std=(0.5,))\n",
            ")\n",
            "\n",
            "Train data:\n",
            "Dataset MNIST\n",
            "    Number of datapoints: 60000\n",
            "    Root location: ./data\n",
            "    Split: Train\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               ToTensor()\n",
            "               Normalize(mean=(0.5,), std=(0.5,))\n",
            "           )\n",
            "\n",
            "Data loader:\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7ff00753de10>\n"
          ]
        }
      ],
      "source": [
        "# Transform\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
        "])\n",
        "print(f\"Transform to be applied:\\n{transform}\\n\")\n",
        "\n",
        "# Load MNIST\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "print(f\"Train data:\\n{train_dataset}\\n\")\n",
        "\n",
        "# Dataloader\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True) # Last batch will be 96 (6000 % 128) instead of 128, so drop to avoid problem with batch norm\n",
        "print(f\"Data loader:\\n{train_loader}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e-FX05bORCr"
      },
      "source": [
        "## Build Model  \n",
        "```Conditional GAN with MLP```  \n",
        "- MLP as neural network  \n",
        "\n",
        "Generator\n",
        "- noise_dim -> hidden layers -> img_dim with ReLU and Tanh  \n",
        "\n",
        "Discriminator\n",
        "- img_dim -> hidden layers -> 1 with Sigmoid activation  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "aOoq4dMUOQFK"
      },
      "outputs": [],
      "source": [
        "# Generator\n",
        "class Generator(nn.Module):\n",
        "  def __init__(self, z_dim, num_classes, img_shape):\n",
        "    super(Generator, self).__init__()\n",
        "    # Set input dimension to include class information. Add Embedding layer to make params learnable.\n",
        "    self.label_emb = nn.Embedding(num_classes, num_classes)\n",
        "    self.img_shape = img_shape\n",
        "    input_dim = z_dim + num_classes\n",
        "\n",
        "    self.model = nn.Sequential(\n",
        "        # MLP\n",
        "        nn.Linear(input_dim, 256),\n",
        "        nn.BatchNorm1d(256),\n",
        "        nn.ReLU(True),\n",
        "\n",
        "        nn.Linear(256, 512),\n",
        "        nn.BatchNorm1d(512),\n",
        "        nn.ReLU(True),\n",
        "\n",
        "        nn.Linear(512, 1024),\n",
        "        nn.BatchNorm1d(1024),\n",
        "        nn.ReLU(True),\n",
        "\n",
        "        nn.Linear(1024, int(torch.prod(torch.tensor(img_shape)))),\n",
        "        nn.Tanh()   # Because we normalized images to [-1, 1]\n",
        "    )\n",
        "\n",
        "  def forward(self, z, labels):\n",
        "    # Concatenate noise and label embedding\n",
        "    x = torch.cat([z, self.label_emb(labels)], dim=1)\n",
        "    img = self.model(x)   # Output shape (784,)\n",
        "    img = img.view(x.size(0), *self.img_shape)  # Output shape (1, 28, 28)\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1LCMEsxRNut",
        "outputId": "e95bb317-5d59-4fd0-82bf-1e0d7b5356e6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Generator(\n",
              "  (label_emb): Embedding(10, 10)\n",
              "  (model): Sequential(\n",
              "    (0): Linear(in_features=110, out_features=256, bias=True)\n",
              "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Linear(in_features=256, out_features=512, bias=True)\n",
              "    (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): Linear(in_features=512, out_features=1024, bias=True)\n",
              "    (7): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): Linear(in_features=1024, out_features=784, bias=True)\n",
              "    (10): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generator = Generator(z_dim=z_dim, num_classes=num_classes, img_shape=img_shape).to(device)\n",
        "generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "T2WF8rteTSIv"
      },
      "outputs": [],
      "source": [
        "# Discriminator\n",
        "class Discriminator(nn.Module):\n",
        "  def __init__(self, num_classes, img_shape):\n",
        "    super(Discriminator, self).__init__()\n",
        "    self.label_emb = nn.Embedding(num_classes, num_classes)\n",
        "    input_dim = int(torch.prod(torch.tensor(img_shape))) + num_classes\n",
        "\n",
        "    self.model = nn.Sequential(\n",
        "        nn.Linear(input_dim, 512),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "        nn.Linear(512, 256),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "        nn.Linear(256, 1),\n",
        "        nn.Sigmoid()    # Outputs probability between 0 and 1\n",
        "    )\n",
        "\n",
        "  def forward(self, img, labels):\n",
        "    # Flatten the image and concatenate label\n",
        "    img_flat = img.view(img.size(0), -1)\n",
        "    x = torch.cat([img_flat, self.label_emb(labels)], dim=1)\n",
        "    return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_14zmS9HUg9w",
        "outputId": "9bf25744-f9d9-478b-d919-a915e2b73b16"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Discriminator(\n",
              "  (label_emb): Embedding(10, 10)\n",
              "  (model): Sequential(\n",
              "    (0): Linear(in_features=794, out_features=512, bias=True)\n",
              "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
              "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (4): Linear(in_features=256, out_features=1, bias=True)\n",
              "    (5): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "discriminator = Discriminator(num_classes, img_shape).to(device)\n",
        "discriminator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "009Y2WceVaRm"
      },
      "source": [
        "## Set up Optimizers  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SO8cadsZVKaE",
        "outputId": "491139f1-d46a-4512-a0d3-245a3c6cf2f9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Adam (\n",
              "Parameter Group 0\n",
              "    amsgrad: False\n",
              "    betas: (0.9, 0.999)\n",
              "    capturable: False\n",
              "    differentiable: False\n",
              "    eps: 1e-08\n",
              "    foreach: None\n",
              "    fused: None\n",
              "    lr: 0.0\n",
              "    maximize: False\n",
              "    weight_decay: 0\n",
              ")"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "g_optim = optim.Adam(generator.parameters(), lr=learning_rate)\n",
        "g_optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hk0BdtVIVwC4",
        "outputId": "56806d5c-cd5c-42a3-971e-0bcb2d7b1074"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Adam (\n",
              "Parameter Group 0\n",
              "    amsgrad: False\n",
              "    betas: (0.9, 0.999)\n",
              "    capturable: False\n",
              "    differentiable: False\n",
              "    eps: 1e-08\n",
              "    foreach: None\n",
              "    fused: None\n",
              "    lr: 0.0\n",
              "    maximize: False\n",
              "    weight_decay: 0\n",
              ")"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "d_optim = optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
        "d_optim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P87c8mF3V8Hm"
      },
      "source": [
        "## Set up Loss functions  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0EQvGQcV7z2",
        "outputId": "759024ae-b9f9-4037-c939-44e11550e26d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BCELoss()"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "criterion = nn.BCELoss()\n",
        "criterion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZA1kqjkPWfVr"
      },
      "source": [
        "## Code for visualizations  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "FK-hef4WWgEd"
      },
      "outputs": [],
      "source": [
        "# def show_generated_images(epoch, generator, fixed_noise):\n",
        "#     generator.eval()\n",
        "#     with torch.no_grad():\n",
        "#         fake_imgs = generator(fixed_noise).reshape(-1, 1, 28, 28)\n",
        "#         fake_imgs = fake_imgs * 0.5 + 0.5  # De-normalize\n",
        "\n",
        "#     grid = torchvision.utils.make_grid(fake_imgs, nrow=8)\n",
        "#     plt.figure(figsize=(8,8))\n",
        "#     plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
        "#     plt.title(f'Generated Images at Epoch {epoch}')\n",
        "#     plt.axis('off')\n",
        "#     plt.show()\n",
        "#     generator.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_n_show_generated_images(epoch, generator, folder_name=folder_name):\n",
        "    generator.eval()\n",
        "    with torch.no_grad():\n",
        "        z = torch.randn(10, z_dim, device=device)\n",
        "        labels = torch.arange(0, 10, dtype=torch.long, device=device)   # Create \n",
        "        samples = generator(z, labels)\n",
        "        samples = samples * 0.5 + 0.5  # Denormalize\n",
        "\n",
        "        # Save the images\n",
        "        save_image(samples, f\"{folder_name}/epoch_{epoch}.png\", nrow=10)\n",
        "\n",
        "        # Display the images\n",
        "        grid = torchvision.utils.make_grid(samples, nrow=10)\n",
        "        plt.figure(figsize=(15, 3))\n",
        "        plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
        "        plt.title(f'Generated Digits (Epoch {epoch})')\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "    generator.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRWGwu29WXRB"
      },
      "source": [
        "## Training loop  \n",
        "- Visualize results every 10 epochs  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYs8k5_cWZtB"
      },
      "outputs": [],
      "source": [
        "def gan_trainer(train_loader, num_epochs=20, discriminator_rounds=1, generator_rounds=1):\n",
        "  # Training loop\n",
        "  for epoch in range(num_epochs):\n",
        "    for batch_idx, (real_images, real_class_labels) in enumerate(train_loader):  # We will be using MNIST class labels for conditional GAN training\n",
        "      batch_size = real_images.size(0)\n",
        "      real_images = real_images.to(device)\n",
        "      real_class_labels = real_class_labels.to(device)\n",
        "\n",
        "      ## Create real and fake labels (note that these are not MNIST class labels. Just labels which tell if image is fake or real)\n",
        "      real_labels = torch.ones(batch_size, 1).to(device)\n",
        "      fake_labels = torch.zeros(batch_size, 1).to(device)\n",
        "\n",
        "      ### ====================================\n",
        "      ### 1. Train Discriminator\n",
        "      ### ====================================\n",
        "      for _ in range(discriminator_rounds):\n",
        "        # Real images and loss\n",
        "        outputs = discriminator(real_images, real_class_labels)\n",
        "        d_loss_real = criterion(outputs, real_labels)\n",
        "\n",
        "        # Fake images and loss\n",
        "        z = torch.randn(batch_size, z_dim).to(device)\n",
        "        fake_class_labels = torch.randint(0, num_classes, (batch_size,)).to(device)\n",
        "        with torch.no_grad():\n",
        "          fake_images = generator(z, fake_class_labels)\n",
        "        outputs = discriminator(fake_images.detach(), fake_class_labels)    # Detach so gradients don't flow into generator\n",
        "        d_loss_fake = criterion(outputs, fake_labels)\n",
        "\n",
        "        # Total discriminator loss\n",
        "        d_loss = d_loss_real + d_loss_fake\n",
        "\n",
        "        # Update Discriminator params\n",
        "        discriminator.zero_grad()\n",
        "        d_loss.backward()\n",
        "        d_optim.step()\n",
        "\n",
        "      ### ====================================\n",
        "      ### 2. Train Generator\n",
        "      ### ====================================\n",
        "      for _ in range(generator_rounds):\n",
        "        # Generate fake images again\n",
        "        z = torch.randn(batch_size, z_dim).to(device)\n",
        "        gen_labels = torch.randint(0, num_classes, (batch_size,)).to(device)\n",
        "        gen_images = generator(z, gen_labels)\n",
        "\n",
        "        # Try to fool the discriminator\n",
        "        outputs = discriminator(gen_images, gen_labels)\n",
        "        g_loss = criterion(outputs, real_labels)  # trick discriminator [# want D(G(z)) = 1]\n",
        "\n",
        "        # Update Generator params\n",
        "        generator.zero_grad()\n",
        "        g_loss.backward()\n",
        "        g_optim.step()\n",
        "\n",
        "\n",
        "    # Visualize\n",
        "    if (epoch+1) % 10 == 0:\n",
        "      print(f\"Epoch: [{epoch+1}/{num_epochs}]\")\n",
        "      print(f\"Size of real images: {real_images.size()}\")\n",
        "      print(f\"Discriminator loss = {d_loss.item():.4f}\")\n",
        "      print(f\"Generator loss = {g_loss.item():.4f}\")\n",
        "      save_n_show_generated_images(epoch=epoch+1, generator=generator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6LP9lJolgCr"
      },
      "source": [
        "## Training  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4HMfPbTQi6wj",
        "outputId": "6f50b218-6eb7-4519-a464-153c66ff6897"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgan_trainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdiscriminator_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiscriminator_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgenerator_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator_rounds\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[38], line 4\u001b[0m, in \u001b[0;36mgan_trainer\u001b[0;34m(train_loader, num_epochs, discriminator_rounds, generator_rounds)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgan_trainer\u001b[39m(train_loader, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, discriminator_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, generator_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      2\u001b[0m   \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[1;32m      3\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (real_images, real_class_labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):  \u001b[38;5;66;03m# We will be using MNIST class labels for conditional GAN training\u001b[39;00m\n\u001b[1;32m      5\u001b[0m       batch_size \u001b[38;5;241m=\u001b[39m real_images\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      6\u001b[0m       real_images \u001b[38;5;241m=\u001b[39m real_images\u001b[38;5;241m.\u001b[39mto(device)\n",
            "File \u001b[0;32m~/hrugved/hrugv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
            "File \u001b[0;32m~/hrugved/hrugv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[0;32m~/hrugved/hrugv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[0;32m~/hrugved/hrugv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[0;32m~/hrugved/hrugv/lib/python3.10/site-packages/torchvision/datasets/mnist.py:146\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    143\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    149\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
            "File \u001b[0;32m~/hrugved/hrugv/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
            "File \u001b[0;32m~/hrugved/hrugv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/hrugved/hrugv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m~/hrugved/hrugv/lib/python3.10/site-packages/torchvision/transforms/transforms.py:277\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    270\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;124;03m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/hrugved/hrugv/lib/python3.10/site-packages/torchvision/transforms/functional.py:327\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnpimg\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Image\u001b[38;5;241m.\u001b[39mfromarray(npimg, mode\u001b[38;5;241m=\u001b[39mmode)\n\u001b[0;32m--> 327\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mnormalize\u001b[39m(tensor: Tensor, mean: List[\u001b[38;5;28mfloat\u001b[39m], std: List[\u001b[38;5;28mfloat\u001b[39m], inplace: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    328\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Normalize a float tensor image with mean and standard deviation.\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;124;03m    This transform does not support PIL Image.\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;124;03m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing():\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "gan_trainer(train_loader,\n",
        "            num_epochs=num_epochs,\n",
        "            discriminator_rounds=discriminator_rounds,\n",
        "            generator_rounds=generator_rounds\n",
        "            )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOxPGwty3MK2+9RoOJ0HT2k",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "hrugv (3.10.12)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
